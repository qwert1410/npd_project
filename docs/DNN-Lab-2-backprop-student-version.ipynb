{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxaMTckGDGgc"
      },
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1_utx_ZGclmCwNttSe40kYA6VHzNocdET' height=\"60\"></center>\n",
        "\n",
        "AI TECH - Akademia Innowacyjnych Zastosowań Technologii Cyfrowych. Program Operacyjny Polska Cyfrowa na lata 2014-2020\n",
        "<hr>\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?id=1BXZ0u3562N_MqCLcekI-Ens77Kk4LpPm'></center>\n",
        "\n",
        "<center>\n",
        "Projekt współfinansowany ze środków Unii Europejskiej w ramach Europejskiego Funduszu Rozwoju Regionalnego\n",
        "Program Operacyjny Polska Cyfrowa na lata 2014-2020,\n",
        "Oś Priorytetowa nr 3 \"Cyfrowe kompetencje społeczeństwa\" Działanie  nr 3.2 \"Innowacyjne rozwiązania na rzecz aktywizacji cyfrowej\"\n",
        "Tytuł projektu:  „Akademia Innowacyjnych Zastosowań Technologii Cyfrowych (AI Tech)”\n",
        "    </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6AieB9jajSq"
      },
      "source": [
        "# Laboratory Scenario 2 - Backpropagation and Gradient Checkpointing\n",
        "\n",
        "In this lab scenario, you are given an implementation of a simple neural network, and your goal is to implement the backpropagation procedure for this network.  \n",
        "To be more precise, the network inputs a tensor $x$ of shape `(MINI_BATCH_SIZE, 28*28)`, where each element of the batch represents a flattened image of shape `(28, 28)`.  \n",
        "In exercise 1, you can assume that elements of the minibatch are fed to the network one by one (as tensors of shape `(1, 28*28)` - single image and `(1, 10)` - image class).  \n",
        "In exercise 2 you are asked to make the backpropagation work without this assumption.  \n",
        "In exercise 3, you will implement a technique called gradient checkpointing, that allows you to reduce the amount of memory used to store activations for backpropagation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6y4l5BmxTNNU"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "from typing import List, Any, Tuple, Optional\n",
        "from numpy.typing import NDArray"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHhqeGLsHcYl",
        "outputId": "323cff7e-4cd4-41c7-c8bd-6055391bf8d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-22 10:22:15--  https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.169.48, 54.231.223.24, 52.217.75.70, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.169.48|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11490434 (11M) [application/octet-stream]\n",
            "Saving to: ‘mnist.npz’\n",
            "\n",
            "mnist.npz           100%[===================>]  10.96M  12.8MB/s    in 0.9s    \n",
            "\n",
            "2024-10-22 10:22:17 (12.8 MB/s) - ‘mnist.npz’ saved [11490434/11490434]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O mnist.npz https://s3.amazonaws.com/img-datasets/mnist.npz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uutaqUkuVAuF"
      },
      "outputs": [],
      "source": [
        "# Let's read the mnist dataset\n",
        "\n",
        "\n",
        "def load_mnist(path=\"mnist.npz\"):\n",
        "    with np.load(path) as f:\n",
        "        x_train, _y_train = f[\"x_train\"], f[\"y_train\"]\n",
        "        x_test, _y_test = f[\"x_test\"], f[\"y_test\"]\n",
        "\n",
        "    x_train = x_train.reshape(-1, 28 * 28) / 255.0\n",
        "    x_test = x_test.reshape(-1, 28 * 28) / 255.0\n",
        "\n",
        "    y_train = np.zeros((_y_train.shape[0], 10))\n",
        "    y_train[np.arange(_y_train.shape[0]), _y_train] = 1\n",
        "\n",
        "    y_test = np.zeros((_y_test.shape[0], 10))\n",
        "    y_test[np.arange(_y_test.shape[0]), _y_test] = 1\n",
        "\n",
        "    return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = load_mnist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5PPE1ldTNNx"
      },
      "source": [
        "## Exercise 1\n",
        "\n",
        "In this exercise, your task is to fill in the gaps in this code by implementing the backpropagation algorithm.\n",
        "Once done, you can run the network on the MNIST example and see how it performs.  \n",
        "Feel free to play with the parameters. Your model should achieve 90%+ accuracy after a few epochs.  \n",
        "\n",
        "Before you start you should note a few things:\n",
        "+ `backprop` - is the function that you need to implement\n",
        "+ `update_mini_batch` - calls `backprop` to get the gradients for network parameters\n",
        "+ The derivative of the loss is already computed by `cost_derivative`.\n",
        "+ Your goal is to compare $\\frac{d L\\left(\\text{model}(x), y\\right)}{d p}$ for each parameter $p$ of the network\n",
        "\n",
        "\n",
        "## Exercise 2 (Optional)\n",
        "\n",
        "Implement a \"fully vectorized\" version, i.e. one using matrix operations instead of going over examples one by one within a minibatch.\n",
        "\n",
        "## Help required?\n",
        "At the end of this notebook, we show how you can utilize `JAX` to check whether you implemented the derivative computation correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "OsCgwvfHTNN0",
        "outputId": "0d6b623f-4d91-4386-85eb-279749ff8968"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-fe6270126517>\u001b[0m in \u001b[0;36m<cell line: 170>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m network.SGD(\n\u001b[0m\u001b[1;32m    171\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-fe6270126517>\u001b[0m in \u001b[0;36mSGD\u001b[0;34m(self, training_data, epochs, mini_batch_size, eta, test_data)\u001b[0m\n\u001b[1;32m    158\u001b[0m                     \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmini_batch_size\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmini_batch_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                 ]\n\u001b[0;32m--> 160\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_mini_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_mini_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_mini_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                 print(\n",
            "\u001b[0;32m<ipython-input-29-fe6270126517>\u001b[0m in \u001b[0;36mupdate_mini_batch\u001b[0;34m(self, x_mini_batch, y_mini_batch, eta)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mnabla_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_mini_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_mini_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             delta_nabla_b, delta_nabla_w = self.backprop(\n\u001b[0m\u001b[1;32m     36\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m784\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             )\n",
            "\u001b[0;32m<ipython-input-29-fe6270126517>\u001b[0m in \u001b[0;36mbackprop\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mzs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def sigmoid(z: NDArray[float]):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "\n",
        "def sigmoid_prime(z: NDArray[float]):\n",
        "    # Derivative of the sigmoid\n",
        "    return sigmoid(z) * (1 - sigmoid(z))\n",
        "\n",
        "\n",
        "class Network(object):\n",
        "    def __init__(self, sizes: List[int]):\n",
        "        # initialize biases and weights with random normal distr.\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.biases = [np.random.randn(y) for y in sizes[1:]]\n",
        "        self.weights = [np.random.randn(x, y) for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "\n",
        "    def feedforward(self, a: NDArray[float]) -> NDArray[float]:\n",
        "        # Run the network on a single case\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(a @ w + b)\n",
        "\n",
        "        return a\n",
        "\n",
        "    def update_mini_batch(\n",
        "        self, x_mini_batch: NDArray[float], y_mini_batch: NDArray[float], eta: float\n",
        "    ) -> None:\n",
        "        # Update network weights and biases by applying a single step\n",
        "        # of gradient descent using backpropagation to compute the gradient.\n",
        "        # The gradient is computed for a mini_batch.\n",
        "        # eta is the learning rate\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        for x, y in zip(x_mini_batch, y_mini_batch):\n",
        "            delta_nabla_b, delta_nabla_w = self.backprop(\n",
        "                x.reshape(1, 784), y.reshape(1, 10)\n",
        "            )\n",
        "            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "\n",
        "        self.weights = [\n",
        "            w - (eta / len(x_mini_batch)) * nw for w, nw in zip(self.weights, nabla_w)\n",
        "        ]\n",
        "        self.biases = [\n",
        "            b - (eta / len(x_mini_batch)) * nb for b, nb in zip(self.biases, nabla_b)\n",
        "        ]\n",
        "\n",
        "    def backprop(\n",
        "        self, x: NDArray[float], y: NDArray[float]\n",
        "    ) -> Tuple[List[NDArray[float]], List[NDArray[float]]]:\n",
        "        # For a single input (x,y) return a tuple of lists.\n",
        "        # First contains gradients over biases, second over weights.\n",
        "\n",
        "        assert len(x.shape) == 2  # batch, features\n",
        "        assert len(y.shape) == 2  # batch, classes\n",
        "        assert x.shape[0] == y.shape[0]\n",
        "\n",
        "        # First initialize the list of gradient arrays\n",
        "        delta_nabla_b = []\n",
        "        delta_nabla_w = []\n",
        "\n",
        "        delta_nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        delta_nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        # Then go forward remembering each layer input and value\n",
        "        # before sigmoid activation\n",
        "        # TODO\n",
        "        ###{\n",
        "        activation = x\n",
        "        activations = [x]\n",
        "        zs = []\n",
        "\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = activation @ w + b\n",
        "            zs.append(z)\n",
        "            activation = sigmoid(z)\n",
        "            activations.append(activation)\n",
        "        ###}\n",
        "\n",
        "        # Now go backward from the final cost applying backpropagation\n",
        "        # hint: you can use reversed(list(zip(a, b, ...)))\n",
        "        # TODO\n",
        "        ###{\n",
        "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
        "\n",
        "        # Gradients for the output layer\n",
        "        delta_nabla_b[-1] = delta.sum(axis=0)\n",
        "        delta_nabla_w[-1] = activations[-2].T @ delta\n",
        "\n",
        "        # Backpropagate the error through the hidden layers\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = sigmoid_prime(z)\n",
        "            delta = delta @ self.weights[-l+1].T * sp  # Backpropagate error\n",
        "            delta_nabla_b[-l] = delta.sum(axis=0)\n",
        "            delta_nabla_w[-l] = activations[-l-1].T @ delta\n",
        "        ###}\n",
        "\n",
        "        # Check shapes\n",
        "        delta_nabla_b = list(delta_nabla_b)\n",
        "        delta_nabla_w = list(delta_nabla_w)\n",
        "        assert len(delta_nabla_b) == len(self.biases), (\n",
        "            len(delta_nabla_b),\n",
        "            len(self.biases),\n",
        "        )\n",
        "        assert len(delta_nabla_w) == len(self.weights), (\n",
        "            len(delta_nabla_w),\n",
        "            len(self.weights),\n",
        "        )\n",
        "        for lid in range(len(self.weights)):\n",
        "            assert delta_nabla_b[lid].shape == self.biases[lid].shape, (\n",
        "                delta_nabla_b[lid].shape,\n",
        "                self.biases[lid].shape,\n",
        "            )\n",
        "            assert delta_nabla_w[lid].shape == self.weights[lid].shape, (\n",
        "                delta_nabla_w[lid].shape,\n",
        "                self.weights[lid].shape,\n",
        "            )\n",
        "\n",
        "        return delta_nabla_b, delta_nabla_w\n",
        "\n",
        "    def evaluate(\n",
        "        self, x_test_data: NDArray[float], y_test_data: NDArray[float]\n",
        "    ) -> float:\n",
        "        # Count the number of correct answers for test_data\n",
        "        test_results = [\n",
        "            (\n",
        "                np.argmax(self.feedforward(x_test_data[i].reshape(1, 784)), axis=-1),\n",
        "                np.argmax(y_test_data[i], axis=-1),\n",
        "            )\n",
        "            for i in range(len(x_test_data))\n",
        "        ]\n",
        "        # return accuracy\n",
        "        return np.mean([int((x == y).item()) for (x, y) in test_results]).item()\n",
        "\n",
        "    def cost_derivative(\n",
        "        self, output_activations: NDArray[float], y: NDArray[float]\n",
        "    ) -> NDArray[float]:\n",
        "        assert output_activations.shape == y.shape, (output_activations.shape, y.shape)\n",
        "        return output_activations - y\n",
        "\n",
        "    def SGD(\n",
        "        self,\n",
        "        training_data: Tuple[NDArray[float], NDArray[float]],\n",
        "        epochs: int,\n",
        "        mini_batch_size: int,\n",
        "        eta: float,\n",
        "        test_data: Optional[Tuple[NDArray[float], NDArray[float]]] = None,\n",
        "    ) -> None:\n",
        "        x_train, y_train = training_data\n",
        "        if test_data:\n",
        "            x_test, y_test = test_data\n",
        "        for j in range(epochs):\n",
        "            for i in range(x_train.shape[0] // mini_batch_size):\n",
        "                x_mini_batch = x_train[\n",
        "                    i * mini_batch_size : (i * mini_batch_size + mini_batch_size)\n",
        "                ]\n",
        "                y_mini_batch = y_train[\n",
        "                    i * mini_batch_size : (i * mini_batch_size + mini_batch_size)\n",
        "                ]\n",
        "                self.update_mini_batch(x_mini_batch, y_mini_batch, eta)\n",
        "            if test_data:\n",
        "                print(\n",
        "                    \"Epoch: {0}, Accuracy: {1}\".format(j, self.evaluate(x_test, y_test))\n",
        "                )\n",
        "            else:\n",
        "                print(\"Epoch: {0}\".format(j))\n",
        "\n",
        "\n",
        "network = Network([784, 30, 10])\n",
        "network.SGD(\n",
        "    (x_train, y_train),\n",
        "    epochs=10,\n",
        "    mini_batch_size=100,\n",
        "    eta=3.0,\n",
        "    test_data=(x_test, y_test),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z: NDArray[float]):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "\n",
        "def sigmoid_prime(z: NDArray[float]):\n",
        "    return sigmoid(z) * (1 - sigmoid(z))\n",
        "\n",
        "\n",
        "class Network(object):\n",
        "    def __init__(self, sizes: List[int]):\n",
        "        # initialize biases and weights with random normal distr.\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]  # Column vectors\n",
        "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "\n",
        "    def feedforward(self, a: NDArray[float]) -> NDArray[float]:\n",
        "        # Run the network on the entire mini-batch\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(w @ a + b)\n",
        "        return a\n",
        "\n",
        "    def update_mini_batch(\n",
        "        self, x_mini_batch: NDArray[float], y_mini_batch: NDArray[float], eta: float\n",
        "    ) -> None:\n",
        "\n",
        "        # Initialize gradients for biases and weights as zeros\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "        delta_nabla_b, delta_nabla_w = self.backprop(x_mini_batch, y_mini_batch)\n",
        "\n",
        "        self.weights = [\n",
        "            w - (eta / x_mini_batch.shape[1]) * nw for w, nw in zip(self.weights, delta_nabla_w)\n",
        "        ]\n",
        "        self.biases = [\n",
        "            b - (eta / x_mini_batch.shape[1]) * nb for b, nb in zip(self.biases, delta_nabla_b)\n",
        "        ]\n",
        "\n",
        "    def backprop(\n",
        "        self, x: NDArray[float], y: NDArray[float]\n",
        "    ) -> Tuple[List[NDArray[float]], List[NDArray[float]]]:\n",
        "\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "        activation = x\n",
        "        activations = [x]\n",
        "        zs = []\n",
        "\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = w @ activation + b\n",
        "            zs.append(z)\n",
        "            activation = sigmoid(z)\n",
        "            activations.append(activation)\n",
        "\n",
        "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
        "        nabla_b[-1] = np.sum(delta, axis=1, keepdims=True)\n",
        "        nabla_w[-1] = delta @ activations[-2].T\n",
        "\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = sigmoid_prime(z)\n",
        "            delta = self.weights[-l+1].T @ delta * sp\n",
        "            nabla_b[-l] = np.sum(delta, axis=1, keepdims=True)\n",
        "            nabla_w[-l] = delta @ activations[-l-1].T\n",
        "\n",
        "        return nabla_b, nabla_w\n",
        "\n",
        "    def evaluate(\n",
        "        self, x_test_data: NDArray[float], y_test_data: NDArray[float]\n",
        "    ) -> float:\n",
        "        # Evaluate using the entire test set\n",
        "        test_results = [\n",
        "            (\n",
        "                np.argmax(self.feedforward(x_test_data[:, i:i+1]), axis=0),\n",
        "                np.argmax(y_test_data[:, i:i+1], axis=0),\n",
        "            )\n",
        "            for i in range(x_test_data.shape[1])\n",
        "        ]\n",
        "        # return accuracy\n",
        "        return np.mean([int(x == y) for (x, y) in test_results])\n",
        "\n",
        "    def cost_derivative(\n",
        "        self, output_activations: NDArray[float], y: NDArray[float]\n",
        "    ) -> NDArray[float]:\n",
        "        # Return the error between predicted and actual output\n",
        "        return output_activations - y\n",
        "\n",
        "    def SGD(\n",
        "        self,\n",
        "        training_data: Tuple[NDArray[float], NDArray[float]],\n",
        "        epochs: int,\n",
        "        mini_batch_size: int,\n",
        "        eta: float,\n",
        "        test_data: Optional[Tuple[NDArray[float], NDArray[float]]] = None,\n",
        "    ) -> None:\n",
        "        x_train, y_train = training_data\n",
        "        if test_data:\n",
        "            x_test, y_test = test_data\n",
        "\n",
        "        x_train = x_train.T\n",
        "        y_train = y_train.T\n",
        "        if test_data:\n",
        "            x_test = x_test.T\n",
        "            y_test = y_test.T\n",
        "\n",
        "        for j in range(epochs):\n",
        "            # Shuffle the training data at the start of each epoch\n",
        "            permutation = np.random.permutation(x_train.shape[1])\n",
        "            x_train = x_train[:, permutation]\n",
        "            y_train = y_train[:, permutation]\n",
        "\n",
        "            # Mini-batch gradient descent\n",
        "            for i in range(0, x_train.shape[1], mini_batch_size):\n",
        "                x_mini_batch = x_train[:, i:i + mini_batch_size]\n",
        "                y_mini_batch = y_train[:, i:i + mini_batch_size]\n",
        "                self.update_mini_batch(x_mini_batch, y_mini_batch, eta)\n",
        "\n",
        "            if test_data:\n",
        "                print(f\"Epoch: {j}, Accuracy: {self.evaluate(x_test, y_test)}\")\n",
        "            else:\n",
        "                print(f\"Epoch: {j}\")\n",
        "\n",
        "\n",
        "network = Network([784, 30, 10])\n",
        "network.SGD(\n",
        "    (x_train, y_train),\n",
        "    epochs=30,\n",
        "    mini_batch_size=100,\n",
        "    eta=3.0,\n",
        "    test_data=(x_test, y_test),\n",
        ")"
      ],
      "metadata": {
        "id": "y57fR5uM2dhX",
        "outputId": "4c7768df-843f-4f03-e672-e8f459cc6168",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-b2e20f9f9681>:91: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  return np.mean([int(x == y) for (x, y) in test_results])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Accuracy: 0.7312\n",
            "Epoch: 1, Accuracy: 0.8218\n",
            "Epoch: 2, Accuracy: 0.8698\n",
            "Epoch: 3, Accuracy: 0.887\n",
            "Epoch: 4, Accuracy: 0.8962\n",
            "Epoch: 5, Accuracy: 0.904\n",
            "Epoch: 6, Accuracy: 0.9081\n",
            "Epoch: 7, Accuracy: 0.9112\n",
            "Epoch: 8, Accuracy: 0.9149\n",
            "Epoch: 9, Accuracy: 0.9164\n",
            "Epoch: 10, Accuracy: 0.9187\n",
            "Epoch: 11, Accuracy: 0.9199\n",
            "Epoch: 12, Accuracy: 0.9214\n",
            "Epoch: 13, Accuracy: 0.9234\n",
            "Epoch: 14, Accuracy: 0.9246\n",
            "Epoch: 15, Accuracy: 0.9272\n",
            "Epoch: 16, Accuracy: 0.9275\n",
            "Epoch: 17, Accuracy: 0.929\n",
            "Epoch: 18, Accuracy: 0.9292\n",
            "Epoch: 19, Accuracy: 0.9305\n",
            "Epoch: 20, Accuracy: 0.9307\n",
            "Epoch: 21, Accuracy: 0.9311\n",
            "Epoch: 22, Accuracy: 0.9311\n",
            "Epoch: 23, Accuracy: 0.9322\n",
            "Epoch: 24, Accuracy: 0.9326\n",
            "Epoch: 25, Accuracy: 0.9329\n",
            "Epoch: 26, Accuracy: 0.9335\n",
            "Epoch: 27, Accuracy: 0.9337\n",
            "Epoch: 28, Accuracy: 0.9343\n",
            "Epoch: 29, Accuracy: 0.9353\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I07OcuTMajSx"
      },
      "source": [
        "# Excercise 3 (optional)\n",
        "\n",
        "The standard backpropagation method requires memorization of all outputs of all layers, which can take much of precious GPU memory.\n",
        "Instead of doing that, one can memorize only a select few layers and then recompute the rest as they are needed.  \n",
        "Your task is to complete the code below to implement backpropagation with checkpoints.\n",
        "To keep things simple, use 1-example mini-batches (or, if you are bored, vectorize the code below)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "4ks-sxtd6VrY"
      },
      "outputs": [],
      "source": [
        "class NetworkWithCheckpoints(object):\n",
        "    def __init__(self, sizes: List[int], checkpoints: List[int]):\n",
        "        # initialize biases and weights with random normal distr.\n",
        "        # weights are indexed by target node first\n",
        "        self.num_layers = len(sizes) - 1\n",
        "        self.sizes = sizes\n",
        "        self.checkpoints = list(\n",
        "            sorted(list(set([0] + checkpoints + [self.num_layers - 1])))\n",
        "        )\n",
        "        self.biases = [np.random.randn(y) for y in sizes[1:]]\n",
        "        self.weights = [np.random.randn(x, y) for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "\n",
        "    def feedforward(self, a: NDArray[float]) -> NDArray[float]:\n",
        "        # Run the network on a single case\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(a @ w + b)\n",
        "        return a\n",
        "\n",
        "    def feedforward_with_checkpoints(\n",
        "        self, x: NDArray[float]\n",
        "    ) -> Tuple[List[NDArray[float]], List[NDArray[float]], NDArray[float]]:\n",
        "        # Runs network on a single case, memorizing the inputs of layers included in checkpoints.\n",
        "        # Notice that gs (outputs of non-linearities) are shifted by one\n",
        "        layer_input = []\n",
        "        before_act = []\n",
        "        for i, (w, b) in enumerate(zip(self.weights, self.biases)):\n",
        "            f = x @ w + b\n",
        "            g = sigmoid(f)\n",
        "            if i in self.checkpoints:\n",
        "                layer_input.append(x)\n",
        "                before_act.append(f)\n",
        "            else:\n",
        "                layer_input.append(None)\n",
        "                before_act.append(None)\n",
        "            x = g\n",
        "        return layer_input, before_act, x\n",
        "\n",
        "    def feedforward_between_layers(\n",
        "        self,\n",
        "        first_layer: int,\n",
        "        last_layer: int,\n",
        "        acc_layer_input: List[NDArray[float]],\n",
        "        acc_before_act: List[NDArray[float]],\n",
        "    ) -> None:\n",
        "        # feedforward input acc_layer_input[first_layer] for layers [first_layer, last_layer)\n",
        "        # memorizing their outputs in respective indexes of acc_layer_input, acc_before_act\n",
        "        # that is for a layer lid \\in {first_layer, ..., last_layer-1}\n",
        "        # acc_layer_input[lid] is the layer input (before linear projection)\n",
        "        # acc_before_act[lid] is the input to sigmoid activation, that is\n",
        "        # acc_before_act[lid] = acc_layer_input[lid] @ self.weights[lid] + self.biases[lid]\n",
        "        # TODO\n",
        "        ###{\n",
        "        layer_input = acc_layer_input[first_layer]\n",
        "        for lid in range(first_layer, last_layer):\n",
        "          acc_layer_input[lid] = layer_input\n",
        "          before_activation = layer_input @ self.weights[lid] + self.biases[lid]\n",
        "          acc_before_act[lid] = before_activation\n",
        "          layer_input = sigmoid(before_activation)\n",
        "        ###}\n",
        "\n",
        "    def backprop_between_layers(\n",
        "        self,\n",
        "        start: int,\n",
        "        end: int,\n",
        "        acc_layer_input: List[NDArray[float]],\n",
        "        acc_before_act: List[NDArray[float]],\n",
        "        dLdg: NDArray[float],\n",
        "    ) -> Tuple[List[NDArray[float]], List[NDArray[float]], NDArray[float]]:\n",
        "        # compute the gradients for layers [start, end)\n",
        "        # dLdg is a gradient with respect to the output (nonlinearity) of layer[end-1]\n",
        "        # return changed dLdG so that it is a gradient with respect to acc_layer_input[start]\n",
        "        # that is the input of layer[start] (in other words output of layer[start - 1])\n",
        "        dLdWs = []\n",
        "        dLdBs = []\n",
        "\n",
        "        # TODO\n",
        "        ###{\n",
        "        for i in range(end - 1, start - 1, -1):\n",
        "          dsigmoid_f = sigmoid_prime(acc_before_act[i])\n",
        "          delta = dLdg * dsigmoid_f\n",
        "          dLdBs.append(delta.mean(axis=0))\n",
        "          dLdW = acc_layer_input[i].T @ delta\n",
        "          dLdWs.append(dLdW)\n",
        "          dLdg = delta @ self.weights[i].T\n",
        "\n",
        "        dLdWs.reverse()\n",
        "        dLdBs.reverse()\n",
        "        ###}\n",
        "\n",
        "        # Checking shapes\n",
        "        dLdWs = list(dLdWs)\n",
        "        dLdBs = list(dLdBs)\n",
        "        assert len(dLdWs) == len(dLdBs), (len(dLdWs), len(dLdBs))\n",
        "        assert len(dLdWs) == end - start, (len(dLdWs), start, end)\n",
        "\n",
        "        for lid in range(start, end):\n",
        "            assert dLdWs[lid - start].shape == self.weights[lid].shape, (\n",
        "                dLdWs[lid - start].shape,\n",
        "                self.weights[lid].shape,\n",
        "            )\n",
        "            assert dLdBs[lid - start].shape == self.biases[lid].shape, (\n",
        "                dLdBs[lid - start].shape,\n",
        "                self.biases[lid].shape,\n",
        "            )\n",
        "\n",
        "        return dLdWs, dLdBs, dLdg\n",
        "\n",
        "    def update_mini_batch(\n",
        "        self, x_mini_batch: NDArray[float], y_mini_batch: NDArray[float], eta: float\n",
        "    ) -> None:\n",
        "        # Update network weights and biases by applying a single step\n",
        "        # of gradient descent using backpropagation with checkpoints to compute the gradient.\n",
        "        # For this exercise, we assume 1 element mini_batch\n",
        "        # eta is the learning rate\n",
        "        x_mini_batch = x_mini_batch.reshape(1, -1)  # batch, features\n",
        "        y_mini_batch = y_mini_batch.reshape(1, -1)\n",
        "\n",
        "        layer_input, before_act, output = self.feedforward_with_checkpoints(\n",
        "            x_mini_batch\n",
        "        )\n",
        "        dLdg = self.cost_derivative(output, y_mini_batch)\n",
        "        for start, end in reversed(\n",
        "            list(\n",
        "                zip(\n",
        "                    self.checkpoints[:-1],\n",
        "                    self.checkpoints[1:][:-1] + [self.checkpoints[-1] + 1],\n",
        "                )\n",
        "            )\n",
        "        ):\n",
        "            # those copies are inefficient, but we do them to keep indexing simple\n",
        "            acc_layer_input = layer_input.copy()\n",
        "            acc_before_act = before_act.copy()\n",
        "            self.feedforward_between_layers(start, end, acc_layer_input, acc_before_act)\n",
        "            nabla_w, nabla_b, dLdg = self.backprop_between_layers(\n",
        "                start, end, acc_layer_input, acc_before_act, dLdg\n",
        "            )\n",
        "            self.weights[start:end] = [\n",
        "                w - eta * dw for w, dw in zip(self.weights[start:end], nabla_w)\n",
        "            ]\n",
        "            self.biases[start:end] = [\n",
        "                b - eta * db for b, db in zip(self.biases[start:end], nabla_b)\n",
        "            ]\n",
        "\n",
        "    def evaluate(\n",
        "        self, x_test_data: NDArray[float], y_test_data: NDArray[float]\n",
        "    ) -> float:\n",
        "        # Count the number of correct answers for test_data\n",
        "        test_results = [\n",
        "            (\n",
        "                np.argmax(self.feedforward(x_test_data[i].reshape(1, 784)), axis=-1),\n",
        "                np.argmax(y_test_data[i], axis=-1),\n",
        "            )\n",
        "            for i in range(len(x_test_data))\n",
        "        ]\n",
        "        # return accuracy\n",
        "        return np.mean([int((x == y).item()) for (x, y) in test_results]).item()\n",
        "\n",
        "    def cost_derivative(\n",
        "        self, output_activations: NDArray[float], y: NDArray[float]\n",
        "    ) -> NDArray[float]:\n",
        "        return output_activations - y\n",
        "\n",
        "    def SGD(\n",
        "        self,\n",
        "        training_data: Tuple[NDArray[float], NDArray[float]],\n",
        "        epochs: int,\n",
        "        mini_batch_size: int,\n",
        "        eta: float,\n",
        "        test_data: Optional[Tuple[NDArray[float], NDArray[float]]] = None,\n",
        "    ):\n",
        "        x_train, y_train = training_data\n",
        "        if test_data:\n",
        "            x_test, y_test = test_data\n",
        "        for j in range(epochs):\n",
        "            for i in range(x_train.shape[0] // mini_batch_size):\n",
        "                x_mini_batch = x_train[\n",
        "                    i * mini_batch_size : (i * mini_batch_size + mini_batch_size)\n",
        "                ]\n",
        "                y_mini_batch = y_train[\n",
        "                    i * mini_batch_size : (i * mini_batch_size + mini_batch_size)\n",
        "                ]\n",
        "                self.update_mini_batch(x_mini_batch, y_mini_batch, eta)\n",
        "            if test_data:\n",
        "                print(\n",
        "                    \"Epoch: {0}, Accuracy: {1}\".format(j, self.evaluate(x_test, y_test))\n",
        "                )\n",
        "            else:\n",
        "                print(\"Epoch: {0}\".format(j))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "mFdDN3NYajSz",
        "outputId": "2649d823-f6d4-4c51-8334-d2ae8fb37b18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Accuracy: 0.4533\n"
          ]
        }
      ],
      "source": [
        "## Debug your solution\n",
        "# correctly implemented checkpointing should give similar results to the non-checkpointed network when seeds are fixed\n",
        "\n",
        "np.random.seed(42)\n",
        "network = NetworkWithCheckpoints([784, 20, 15, 10, 10], checkpoints=[2])\n",
        "network.SGD(\n",
        "    (x_train, y_train),\n",
        "    epochs=1,\n",
        "    mini_batch_size=1,\n",
        "    eta=0.02,\n",
        "    test_data=(x_test, y_test),\n",
        ")\n",
        "\n",
        "np.random.seed(42)\n",
        "network = Network([784, 20, 15, 10, 10])\n",
        "network.SGD(\n",
        "    (x_train, y_train),\n",
        "    epochs=1,\n",
        "    mini_batch_size=1,\n",
        "    eta=0.02,\n",
        "    test_data=(x_test, y_test),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "collapsed": true,
        "id": "wmPH7bGQajSz",
        "outputId": "d83c2a7d-19c0-4177-b76b-2fdcde1c0ec7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Accuracy: 0.8403\n",
            "Epoch: 1, Accuracy: 0.8881\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-10ec4319aa09>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNetworkWithCheckpoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoints\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m network.SGD(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-1b700132e759>\u001b[0m in \u001b[0;36mSGD\u001b[0;34m(self, training_data, epochs, mini_batch_size, eta, test_data)\u001b[0m\n\u001b[1;32m    180\u001b[0m                     \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmini_batch_size\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmini_batch_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 ]\n\u001b[0;32m--> 182\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_mini_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_mini_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_mini_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 print(\n",
            "\u001b[0;32m<ipython-input-22-1b700132e759>\u001b[0m in \u001b[0;36mupdate_mini_batch\u001b[0;34m(self, x_mini_batch, y_mini_batch, eta)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0my_mini_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_mini_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         layer_input, before_act, output = self.feedforward_with_checkpoints(\n\u001b[0m\u001b[1;32m    119\u001b[0m             \u001b[0mx_mini_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         )\n",
            "\u001b[0;32m<ipython-input-22-1b700132e759>\u001b[0m in \u001b[0;36mfeedforward_with_checkpoints\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoints\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mlayer_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-b2e20f9f9681>\u001b[0m in \u001b[0;36msigmoid\u001b[0;34m(z)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNDArray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msigmoid_prime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNDArray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "network = NetworkWithCheckpoints([784, 30, 30, 10], checkpoints=[1])\n",
        "network.SGD(\n",
        "    (x_train, y_train),\n",
        "    epochs=5,\n",
        "    mini_batch_size=1,\n",
        "    eta=0.05,\n",
        "    test_data=(x_test, y_test),\n",
        ")  # per-example descend is really slow, try vectorizing it!\n",
        "# Just so you know, un-vectorized version takes about 25-35s per epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cn-hvzUKajS0"
      },
      "source": [
        "# JAX Playground (Optional)\n",
        "JAX is a framework that allows the creation of neural networks with numpy-like syntax.  \n",
        "In this course, we will use Pytorch instead of JAX, but for this lab scenario, JAX can help us test our gradient computation implementation.  \n",
        "Let's give it a try  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "qcsbs3JOajS0",
        "outputId": "02a7eab3-0945-4b3d-a88e-4bff8996d70c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (0.4.33)\n",
            "Requirement already satisfied: jaxlib<=0.4.33,>=0.4.33 in /usr/local/lib/python3.10/dist-packages (from jax) (0.4.33)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from jax) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax) (1.13.1)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install jax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "4DGTolLYajS0",
        "outputId": "d50c0b8c-2d8d-4506-f34b-ee05d1233937",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'dx' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-3476fb3cb0a3>\u001b[0m in \u001b[0;36m<cell line: 48>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mjax_dx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjax_dw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjax_db\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mdx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmanual_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m print(\n",
            "\u001b[0;32m<ipython-input-32-3476fb3cb0a3>\u001b[0m in \u001b[0;36mmanual_backward\u001b[0;34m(x, w, b)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m###}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dx' is not defined"
          ]
        }
      ],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "\n",
        "def sigmoid(z: jax.Array):\n",
        "    return 1.0 / (1.0 + jnp.exp(-z))\n",
        "\n",
        "\n",
        "def sigmoid_prime(z: NDArray[float]):\n",
        "    return sigmoid(z) * (1 - sigmoid(z))\n",
        "\n",
        "\n",
        "key = jax.random.key(42)\n",
        "\n",
        "key, subkey = jax.random.split(key)\n",
        "w = jax.random.normal(subkey, (5, 5))\n",
        "key, subkey = jax.random.split(key)\n",
        "b = jax.random.normal(subkey, (5,))\n",
        "x = jnp.arange(5, dtype=w.dtype).reshape(1, 5)\n",
        "\n",
        "\n",
        "# Define a jax function\n",
        "# We emphasize that function (not procedure)\n",
        "# In fact there are more requirements for writing good\n",
        "# jax code but this is just an example (see https://jax.readthedocs.io/en/latest/tutorials.html)\n",
        "def forward(x: jax.Array, w: jax.Array, b: jax.Array) -> jax.Array:\n",
        "    f = x @ w + b\n",
        "    g = sigmoid(f)\n",
        "    loss = g.sum()\n",
        "    return loss, g\n",
        "\n",
        "\n",
        "# this will calculate gradient for first, second, and third argument\n",
        "# has_aux tells that in addition to loss our function returns something else\n",
        "forward_backward = jax.value_and_grad(fun=forward, argnums=[0, 1, 2], has_aux=True)\n",
        "\n",
        "\n",
        "def manual_backward(x, w, b):\n",
        "    ## TODO\n",
        "    ###{\n",
        "    pass\n",
        "    ###}\n",
        "    return dx, dw, db\n",
        "\n",
        "\n",
        "(loss, res), grad = forward_backward(x, w, b)\n",
        "jax_dx, jax_dw, jax_db = grad\n",
        "dx, dw, db = manual_backward(x, w, b)\n",
        "\n",
        "print(\n",
        "    f\"\"\"\n",
        "diff dx = {jnp.mean(jnp.abs(jax_dx - dx))}\n",
        "diff dw = {jnp.mean(jnp.abs(jax_dw - dw))}\n",
        "diff db = {jnp.mean(jnp.abs(jax_db - db))}\n",
        "emach = {np.finfo(dx.dtype).eps}\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2InpP0r1Bh55"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}